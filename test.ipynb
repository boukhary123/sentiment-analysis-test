{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNqwNKGpQqmIFYvJRNOjR1v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1101adf624e04775930b0858cc549122":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d084d3b6876145b1857aa87e3bf96ce1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_38b8b91c88444ad48f210f2f73517d01","IPY_MODEL_818ebbe008a6476fbe6f55770b620cac","IPY_MODEL_e7343628995344848a011590c1a9a39d"]}},"d084d3b6876145b1857aa87e3bf96ce1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"38b8b91c88444ad48f210f2f73517d01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_80e7c8a29f3c427cb76df7a6398cdb11","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b8b380946cfd423b8655157ae25863cc"}},"818ebbe008a6476fbe6f55770b620cac":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c51a19bdcccc45458cd9d0e5556b0624","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":20,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dc108ea676ae41d79f763685b96114ff"}},"e7343628995344848a011590c1a9a39d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c5c7c09f7d1d4a74b875317668129e54","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20/20 [00:03&lt;00:00,  6.35ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c062cfd8c6c34a208c8d92d33d047d15"}},"80e7c8a29f3c427cb76df7a6398cdb11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b8b380946cfd423b8655157ae25863cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c51a19bdcccc45458cd9d0e5556b0624":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"dc108ea676ae41d79f763685b96114ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c5c7c09f7d1d4a74b875317668129e54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c062cfd8c6c34a208c8d92d33d047d15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f62779a79b2c470a9fb3b1ad2fd5c6f1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_099a275d538a4bf297fd88e34bbd733e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_7a51209e898342b283b41bbc5b3a0ffe","IPY_MODEL_131becf75579425b9c7a66e1e45b733e","IPY_MODEL_c42d1fdab6e1420c988627855cbe3fc3"]}},"099a275d538a4bf297fd88e34bbd733e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a51209e898342b283b41bbc5b3a0ffe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2682ceb31a4d4b2fb3eaf02389cb9625","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_77a04acbd6f9453f8dbb381d000e90c9"}},"131becf75579425b9c7a66e1e45b733e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_ef986bec4e4948a6a771b9215d684830","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_57f05853bb4c426ab522790abc3710aa"}},"c42d1fdab6e1420c988627855cbe3fc3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1e710442b2c2402d8cd8e4b43c707cc5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5/5 [00:00&lt;00:00,  5.97ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b2481444c4f940a885e429ab1640b473"}},"2682ceb31a4d4b2fb3eaf02389cb9625":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"77a04acbd6f9453f8dbb381d000e90c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef986bec4e4948a6a771b9215d684830":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"57f05853bb4c426ab522790abc3710aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1e710442b2c2402d8cd8e4b43c707cc5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b2481444c4f940a885e429ab1640b473":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90a9b0a97c314b09b564b13b218a3602":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_22ff10ab48fd48499f13aad52071b540","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1a94b1a5befd4c138461668b1d643307","IPY_MODEL_304de32a92434ce49478d2fdb1e1ad84","IPY_MODEL_86385b48dd564cadbf48da96b74a1d81"]}},"22ff10ab48fd48499f13aad52071b540":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a94b1a5befd4c138461668b1d643307":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_29dfdaccea034feb9bee59a97056581b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dbe696d193c74d66a20c92ff5695326f"}},"304de32a92434ce49478d2fdb1e1ad84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_bbd8ca72f27e41c3826ed525dfffdabb","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":20,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c9eae018e9ba4ed7865c30ca96492f74"}},"86385b48dd564cadbf48da96b74a1d81":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_68e97e67d4fd43e08b2be536f18c6d8a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20/20 [00:03&lt;00:00,  5.42ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_3706c07caed44710a135ef9fae31ac8d"}},"29dfdaccea034feb9bee59a97056581b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dbe696d193c74d66a20c92ff5695326f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bbd8ca72f27e41c3826ed525dfffdabb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"c9eae018e9ba4ed7865c30ca96492f74":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"68e97e67d4fd43e08b2be536f18c6d8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"3706c07caed44710a135ef9fae31ac8d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"99125865325d42cabaa6d50a96015e3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4011a14e9b4c40b2b16df82735186c0d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_96cdc291da8d4233860bf0ae2e070e9b","IPY_MODEL_04a070f2f6ad4cf593acd2e1652e0df8","IPY_MODEL_515dbebdfa9242f0b335115a6e5d3b90"]}},"4011a14e9b4c40b2b16df82735186c0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96cdc291da8d4233860bf0ae2e070e9b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e702f834550f410489a69a5bb8ea3b4a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_348c10d1bd56453784a1c06aa92de8bf"}},"04a070f2f6ad4cf593acd2e1652e0df8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b441ebe3a9954547b705cb9400440cf2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0b15c15df8a446ba8438e18a78774d7a"}},"515dbebdfa9242f0b335115a6e5d3b90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_847bad8fb5544571a9f74c222635c7aa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5/5 [00:00&lt;00:00,  5.15ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_61a7c2c6db0046e381a6c0ca4d47c007"}},"e702f834550f410489a69a5bb8ea3b4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"348c10d1bd56453784a1c06aa92de8bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b441ebe3a9954547b705cb9400440cf2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0b15c15df8a446ba8438e18a78774d7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"847bad8fb5544571a9f74c222635c7aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"61a7c2c6db0046e381a6c0ca4d47c007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e14477cd75cd4f5398f7f91d06832917":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_062f970068794bd6a835094e77b7ff78","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2ffe003e7f744052ac595e46ccf90b43","IPY_MODEL_dfd58e8082d14fbe8f8f8065af5d0973","IPY_MODEL_87ff94e5575b4c4392a17ca9b66a284b"]}},"062f970068794bd6a835094e77b7ff78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2ffe003e7f744052ac595e46ccf90b43":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_d0fd4e5692bd471d8d98c1575bed06e5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f6682fafb594472faa7c42eaf87262f6"}},"dfd58e8082d14fbe8f8f8065af5d0973":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_766af986723149cbadbe9fd7e3606b0c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":20,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":20,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d592cd9e1b564a7ea540c8fb7cf9d250"}},"87ff94e5575b4c4392a17ca9b66a284b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3878949df4f3442bbc7f7d5f01e88ddc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 20/20 [00:04&lt;00:00,  5.49ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a2a1acb627da4c9284fc80afa6c8dfcd"}},"d0fd4e5692bd471d8d98c1575bed06e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f6682fafb594472faa7c42eaf87262f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"766af986723149cbadbe9fd7e3606b0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d592cd9e1b564a7ea540c8fb7cf9d250":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3878949df4f3442bbc7f7d5f01e88ddc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a2a1acb627da4c9284fc80afa6c8dfcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7c7d80ff456e4df88ab0e19f884bbe07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ff852b8ec39b43af88ff3a64910f5237","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a82b176bfb574a34a92e5f2f8fa70b54","IPY_MODEL_20d48d1da9474698aeb397e2bfda10d5","IPY_MODEL_2cc06eed6ba74d34ac90d81cc69d9902"]}},"ff852b8ec39b43af88ff3a64910f5237":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a82b176bfb574a34a92e5f2f8fa70b54":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_50eeb10a9987476c9ba7339a63c81f4f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f3c4fe105ed54b3d877c1049f2674866"}},"20d48d1da9474698aeb397e2bfda10d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_eab47f1b8d294cf19f08f54563bce15a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":5,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_676b47918a2b46a7842c204855b345a1"}},"2cc06eed6ba74d34ac90d81cc69d9902":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_cb068f341a964166a5a8de54ee39780d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5/5 [00:00&lt;00:00,  5.39ba/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f7c2f504df36425f89ab83622b0b93f9"}},"50eeb10a9987476c9ba7339a63c81f4f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f3c4fe105ed54b3d877c1049f2674866":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"eab47f1b8d294cf19f08f54563bce15a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"676b47918a2b46a7842c204855b345a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cb068f341a964166a5a8de54ee39780d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f7c2f504df36425f89ab83622b0b93f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85cbceee500448d7ae3a59e1972a1d6c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_006dc4366de64ac59e926025b2fadb76","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1bbc5057749d4237afb45abd32dcc7f2","IPY_MODEL_f1278fa2a5124f1894843b319d298c99","IPY_MODEL_717cd88ab73e4d3fada3e48a51743f17"]}},"006dc4366de64ac59e926025b2fadb76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1bbc5057749d4237afb45abd32dcc7f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_628517d2553b41f3a2bbda8b2487dd35","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5ee9ade599c640d79f3c3f5579696907"}},"f1278fa2a5124f1894843b319d298c99":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_542634e593e743748d8b4c526d7cf250","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f582367a8fd2499eaf9f26fb4e4f2e68"}},"717cd88ab73e4d3fada3e48a51743f17":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_368ed9f75ddc44a593a76e4ae3f2599d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:00&lt;00:00, 34.02it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b3ca776a82a4553ac07d8d60b419068"}},"628517d2553b41f3a2bbda8b2487dd35":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5ee9ade599c640d79f3c3f5579696907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"542634e593e743748d8b4c526d7cf250":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"f582367a8fd2499eaf9f26fb4e4f2e68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"368ed9f75ddc44a593a76e4ae3f2599d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8b3ca776a82a4553ac07d8d60b419068":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"wjrwdWj4ww6s"},"source":["# Installs"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"InSMI2PzBL1U","executionInfo":{"status":"ok","timestamp":1638456482713,"user_tz":300,"elapsed":16577,"user":{"displayName":"Alaa Boukhary","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11668845938527693120"}},"outputId":"9d68be3b-c87b-4257-b0a7-995f32dbd5a7"},"source":["!pip install sentence-transformers\n","!pip install transformers\n","!pip install datasets"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting sentence-transformers\n","  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 5.3 MB/s \n","\u001b[?25hCollecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 32.5 MB/s \n","\u001b[?25hCollecting tokenizers>=0.10.3\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 49.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 71.8 MB/s \n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.2.0-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 717 kB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 64.5 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 69.2 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (7.1.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.0.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=c1f8ddf6bec8ebfd304a34eba187bf789a8bd7429d7dd811af6210632811fe6e\n","  Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad\n","Successfully built sentence-transformers\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.0 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.5\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Collecting datasets\n","  Downloading datasets-1.16.1-py3-none-any.whl (298 kB)\n","\u001b[K     |████████████████████████████████| 298 kB 15.0 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.2.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.2)\n","Collecting xxhash\n","  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n","\u001b[K     |████████████████████████████████| 243 kB 61.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2021.11.1-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 76.4 MB/s \n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 67.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 76.8 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.8)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.1-py3-none-any.whl (5.7 kB)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (192 kB)\n","\u001b[K     |████████████████████████████████| 192 kB 80.0 MB/s \n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n","\u001b[K     |████████████████████████████████| 160 kB 75.1 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, datasets\n","Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.1 asynctest-0.13.0 datasets-1.16.1 frozenlist-1.2.0 fsspec-2021.11.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MiPkr7iMBW4t","executionInfo":{"status":"ok","timestamp":1638456543483,"user_tz":300,"elapsed":27531,"user":{"displayName":"Alaa Boukhary","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11668845938527693120"}},"outputId":"39e3238e-7974-4ee3-e4af-d4ffcb3bc55a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"V46cdd9jw7Zw"},"source":["# Loading Data and Training"]},{"cell_type":"markdown","metadata":{"id":"8UNAerKgordp"},"source":["I chose to use transformers since they been have shown to be state of the art in many down stream tasks. Plus the availibility of pretrained multilingual models for transformers makes the choice more complelling and will facilitate the training/finetuning process. I first chose distillbert as it is a light weight Bert-like model which performs much faster than BERT and according to author provides more accuracy on some Benchmarks. XLM-Roberta also is faster and is more lightweight than BERT so I decided to test it as well. The models can be trained for more epochs, and could achieve higher accuracy, but due to time constraint and since the point of this challenge is just proof of concept I ran traning for only one epoch. distilbert and XLM-Roberta were trained in an unsupervised manner (masked word prediction) from crawled data. There exists also some open source models which were finetuned on the specific task of sentimenet analysis. One of the few models is called twitter-xlm-roberta. This is a multilingual model which was finetuned on twitter data for sentiment analysis. Usually twitter data will reflect more how people talk instead of for example wikipedia so it should capture the sentimenets with higher accuracy, plus it's already finetuned for this task. So we will try twitter-xlm-roberta as well.\n","\n","In addition, there is one class which unassigned so I just drop all utterances which belong to this class."]},{"cell_type":"code","metadata":{"id":"kHH-Z1-ABmYb","executionInfo":{"status":"ok","timestamp":1638456588074,"user_tz":300,"elapsed":8443,"user":{"displayName":"Alaa Boukhary","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11668845938527693120"}}},"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from datasets import ClassLabel\n","from datasets import Dataset\n","from transformers import AutoModelForSequenceClassification\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","import numpy as np\n","from datasets import load_metric\n","import torch\n","import pandas as pd\n","\n","labels = ClassLabel(num_classes=3, names=['negative', 'positive','neutral'])\n","\n","def tokenize_train_function(examples):\n","    tokens = tokenizer(examples[\"content\"], max_length= 512, padding=\"max_length\", truncation=True)\n","    tokens['label'] = labels.str2int(examples['label'])\n","    return tokens\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels,average=\"weighted\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["1101adf624e04775930b0858cc549122","d084d3b6876145b1857aa87e3bf96ce1","38b8b91c88444ad48f210f2f73517d01","818ebbe008a6476fbe6f55770b620cac","e7343628995344848a011590c1a9a39d","80e7c8a29f3c427cb76df7a6398cdb11","b8b380946cfd423b8655157ae25863cc","c51a19bdcccc45458cd9d0e5556b0624","dc108ea676ae41d79f763685b96114ff","c5c7c09f7d1d4a74b875317668129e54","c062cfd8c6c34a208c8d92d33d047d15","f62779a79b2c470a9fb3b1ad2fd5c6f1","099a275d538a4bf297fd88e34bbd733e","7a51209e898342b283b41bbc5b3a0ffe","131becf75579425b9c7a66e1e45b733e","c42d1fdab6e1420c988627855cbe3fc3","2682ceb31a4d4b2fb3eaf02389cb9625","77a04acbd6f9453f8dbb381d000e90c9","ef986bec4e4948a6a771b9215d684830","57f05853bb4c426ab522790abc3710aa","1e710442b2c2402d8cd8e4b43c707cc5","b2481444c4f940a885e429ab1640b473","90a9b0a97c314b09b564b13b218a3602","22ff10ab48fd48499f13aad52071b540","1a94b1a5befd4c138461668b1d643307","304de32a92434ce49478d2fdb1e1ad84","86385b48dd564cadbf48da96b74a1d81","29dfdaccea034feb9bee59a97056581b","dbe696d193c74d66a20c92ff5695326f","bbd8ca72f27e41c3826ed525dfffdabb","c9eae018e9ba4ed7865c30ca96492f74","68e97e67d4fd43e08b2be536f18c6d8a","3706c07caed44710a135ef9fae31ac8d","99125865325d42cabaa6d50a96015e3d","4011a14e9b4c40b2b16df82735186c0d","96cdc291da8d4233860bf0ae2e070e9b","04a070f2f6ad4cf593acd2e1652e0df8","515dbebdfa9242f0b335115a6e5d3b90","e702f834550f410489a69a5bb8ea3b4a","348c10d1bd56453784a1c06aa92de8bf","b441ebe3a9954547b705cb9400440cf2","0b15c15df8a446ba8438e18a78774d7a","847bad8fb5544571a9f74c222635c7aa","61a7c2c6db0046e381a6c0ca4d47c007","e14477cd75cd4f5398f7f91d06832917","062f970068794bd6a835094e77b7ff78","2ffe003e7f744052ac595e46ccf90b43","dfd58e8082d14fbe8f8f8065af5d0973","87ff94e5575b4c4392a17ca9b66a284b","d0fd4e5692bd471d8d98c1575bed06e5","f6682fafb594472faa7c42eaf87262f6","766af986723149cbadbe9fd7e3606b0c","d592cd9e1b564a7ea540c8fb7cf9d250","3878949df4f3442bbc7f7d5f01e88ddc","a2a1acb627da4c9284fc80afa6c8dfcd","7c7d80ff456e4df88ab0e19f884bbe07","ff852b8ec39b43af88ff3a64910f5237","a82b176bfb574a34a92e5f2f8fa70b54","20d48d1da9474698aeb397e2bfda10d5","2cc06eed6ba74d34ac90d81cc69d9902","50eeb10a9987476c9ba7339a63c81f4f","f3c4fe105ed54b3d877c1049f2674866","eab47f1b8d294cf19f08f54563bce15a","676b47918a2b46a7842c204855b345a1","cb068f341a964166a5a8de54ee39780d","f7c2f504df36425f89ab83622b0b93f9"]},"id":"6Yc6q8TdJIoL","executionInfo":{"status":"ok","timestamp":1638460593835,"user_tz":300,"elapsed":3725129,"user":{"displayName":"Alaa Boukhary","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11668845938527693120"}},"outputId":"e059deec-7091-4d6a-90ea-764bfa519c3e"},"source":["from transformers import AutoModelForSequenceClassification\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","import numpy as np\n","from datasets import load_metric\n","\n","\n","\n","model_types = [\"distilbert-base-multilingual-cased\",\"xlm-roberta-base\",\"cardiffnlp/twitter-xlm-roberta-base-sentiment\"]\n","data_path = \"/content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/data/\"\n","model_path = \"/content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/\"\n","train_df = pd.read_csv(data_path + \"train.csv\")\n","train_df = train_df[train_df[\"sentiment\"]!=\"unassigned\"]\n","train_dataset = Dataset.from_pandas(train_df).rename_column(\"sentiment\", \"label\").train_test_split(test_size=0.2)\n","metric = load_metric(\"f1\")\n","scores = {}\n","runned_models = {}\n","for model_type in model_types:\n","  tokenizer = AutoTokenizer.from_pretrained(model_type)\n","  tokenized_train_datasets = train_dataset.map(tokenize_train_function, batched=True)\n","\n","  # small_train_dataset = tokenized_train_datasets['train'].shuffle(seed=42).select(range(100))\n","  # small_eval_dataset = tokenized_train_datasets[\"test\"].shuffle(seed=42).select(range(100))\n","  full_train_dataset = tokenized_train_datasets[\"train\"]\n","  full_eval_dataset = tokenized_train_datasets[\"test\"]\n","\n","  model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=3)\n","  training_args = TrainingArguments(\"test_trainer\", num_train_epochs=1.0)\n","  trainer = Trainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=full_train_dataset,\n","      eval_dataset=full_eval_dataset,\n","      compute_metrics=compute_metrics\n","  )\n","  trainer.train()\n","  model.save_pretrained(save_directory=model_path + model_type)\n","  scores[model_type] = trainer.evaluate()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.5\",\n","  \"vocab_size\": 119547\n","}\n","\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.5\",\n","  \"vocab_size\": 119547\n","}\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1101adf624e04775930b0858cc549122","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/20 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f62779a79b2c470a9fb3b1ad2fd5c6f1","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.5\",\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/7b48683e2e7ba71cd1d7d6551ac325eceee01db5c2f3e81cfbfd1ee7bb7877f2.c24097b0cf91dbc66977325325fd03112f0f13d0e3579abbffc8d1e45f8d0619\n","Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['pre_classifier.bias', 'classifier.bias', 'classifier.weight', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: content, __index_level_0__.\n","***** Running training *****\n","  Num examples = 19999\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2500\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2500/2500 10:47, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.692400</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.579200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.517300</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.490100</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.461600</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to test_trainer/checkpoint-500\n","Configuration saved in test_trainer/checkpoint-500/config.json\n","Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-1000\n","Configuration saved in test_trainer/checkpoint-1000/config.json\n","Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-1500\n","Configuration saved in test_trainer/checkpoint-1500/config.json\n","Model weights saved in test_trainer/checkpoint-1500/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-2000\n","Configuration saved in test_trainer/checkpoint-2000/config.json\n","Model weights saved in test_trainer/checkpoint-2000/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-2500\n","Configuration saved in test_trainer/checkpoint-2500/config.json\n","Model weights saved in test_trainer/checkpoint-2500/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/distilbert-base-multilingual-cased/config.json\n","Model weights saved in /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/distilbert-base-multilingual-cased/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: content, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 5000\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [625/625 00:50]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading file https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/9df9ae4442348b73950203b63d1b8ed2d18eba68921872aee0c3a9d05b9673c6.00628a9eeb8baf4080d44a0abe9fe8057893de20c7cb6e6423cddbf452f7d4d8\n","loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/daeda8d936162ca65fe6dd158ecce1d8cb56c17d89b78ab86be1558eaef1d76a.a984cf52fc87644bd4a2165f1e07e0ac880272c1e82d648b4674907056912bd7\n","loading file https://huggingface.co/xlm-roberta-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/xlm-roberta-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/xlm-roberta-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90a9b0a97c314b09b564b13b218a3602","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/20 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99125865325d42cabaa6d50a96015e3d","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/xlm-roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/87683eb92ea383b0475fecf99970e950a03c9ff5e51648d6eee56fb754612465.ab95cf27f9419a99cce4f19d09e655aba382a2bafe2fe26d0cc24c18cf1a1af6\n","Model config XLMRobertaConfig {\n","  \"architectures\": [\n","    \"XLMRobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/xlm-roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/97d0ea09f8074264957d062ec20ccb79af7b917d091add8261b26874daf51b5d.f42212747c1c27fcebaa0a89e2a83c38c6d3d4340f21922f892b88d882146ac2\n","Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.bias']\n","- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: content, __index_level_0__.\n","***** Running training *****\n","  Num examples = 19999\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2500\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2500/2500 22:28, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1.108600</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.104400</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.102100</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.103300</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.100300</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to test_trainer/checkpoint-500\n","Configuration saved in test_trainer/checkpoint-500/config.json\n","Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-1000\n","Configuration saved in test_trainer/checkpoint-1000/config.json\n","Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-1500\n","Configuration saved in test_trainer/checkpoint-1500/config.json\n","Model weights saved in test_trainer/checkpoint-1500/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-2000\n","Configuration saved in test_trainer/checkpoint-2000/config.json\n","Model weights saved in test_trainer/checkpoint-2000/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-2500\n","Configuration saved in test_trainer/checkpoint-2500/config.json\n","Model weights saved in test_trainer/checkpoint-2500/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/xlm-roberta-base/config.json\n","Model weights saved in /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/xlm-roberta-base/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: content, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 5000\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [625/625 01:38]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/sentencepiece.bpe.model from cache at /root/.cache/huggingface/transformers/fa0936a0eed8b226fb1679d5c7062921e8e84e3d1536ff26b5ed08bdcd444d93.71e50b08dbe7e5375398e165096cacc3d2086119d6a449364490da6908de655e\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/tokenizer.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/6654a835c284613a15c3b583fce96f417606b95fab5ef47cc3da33de8ac237b6.0dc5b1041f62041ebbd23b1297f2f573769d5c97d8b7c28180ec86b8f6185aa8\n","loading file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e14477cd75cd4f5398f7f91d06832917","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/20 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c7d80ff456e4df88ab0e19f884bbe07","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/5 [00:00<?, ?ba/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/9628a03bf91a381b0f93e02e13ed34077a805ede6a568ad868817f87437a55ea.ea50decabb7db740257ca1cdefd63c25ffafb958ec595a0ff0c8dbac3f4b1ae6\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"/home/jupyter/misc/tweeteval/TweetEval_models/xlm-twitter/local-twitter-xlm-roberta-base-sentiment/\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4356cbb44d246494fb9361d024371aee59c2fc67b648f4f964f1dbb2ba53e5b5.a22545cd79d055b2220db85f4707145de60be10ed4b5cdebfe0bd19b5a8c3a43\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the training set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: content, __index_level_0__.\n","***** Running training *****\n","  Num examples = 19999\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 8\n","  Total train batch size (w. parallel, distributed & accumulation) = 8\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 2500\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2500/2500 22:28, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.592800</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.465800</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.455500</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.433800</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.363500</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to test_trainer/checkpoint-500\n","Configuration saved in test_trainer/checkpoint-500/config.json\n","Model weights saved in test_trainer/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-1000\n","Configuration saved in test_trainer/checkpoint-1000/config.json\n","Model weights saved in test_trainer/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-1500\n","Configuration saved in test_trainer/checkpoint-1500/config.json\n","Model weights saved in test_trainer/checkpoint-1500/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-2000\n","Configuration saved in test_trainer/checkpoint-2000/config.json\n","Model weights saved in test_trainer/checkpoint-2000/pytorch_model.bin\n","Saving model checkpoint to test_trainer/checkpoint-2500\n","Configuration saved in test_trainer/checkpoint-2500/config.json\n","Model weights saved in test_trainer/checkpoint-2500/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Configuration saved in /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/config.json\n","Model weights saved in /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/pytorch_model.bin\n","The following columns in the evaluation set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: content, __index_level_0__.\n","***** Running Evaluation *****\n","  Num examples = 5000\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [625/625 01:38]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AuPy4MwohoxR","executionInfo":{"status":"ok","timestamp":1638461122505,"user_tz":300,"elapsed":358,"user":{"displayName":"Alaa Boukhary","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11668845938527693120"}},"outputId":"9aed3420-5ef2-4978-be3e-e053f2856320"},"source":["scores"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'cardiffnlp/twitter-xlm-roberta-base-sentiment': {'epoch': 1.0,\n","  'eval_f1': 0.8709104072682577,\n","  'eval_loss': 0.3887748420238495,\n","  'eval_runtime': 98.8391,\n","  'eval_samples_per_second': 50.587,\n","  'eval_steps_per_second': 6.323},\n"," 'distilbert-base-multilingual-cased': {'epoch': 1.0,\n","  'eval_f1': 0.8195441519707125,\n","  'eval_loss': 0.4402909278869629,\n","  'eval_runtime': 51.0261,\n","  'eval_samples_per_second': 97.989,\n","  'eval_steps_per_second': 12.249},\n"," 'xlm-roberta-base': {'epoch': 1.0,\n","  'eval_f1': 0.18528672278836217,\n","  'eval_loss': 1.0975550413131714,\n","  'eval_runtime': 98.6036,\n","  'eval_samples_per_second': 50.708,\n","  'eval_steps_per_second': 6.339}}"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"uKVtdp1swmKM"},"source":["# Generate Predictions with Best model Found"]},{"cell_type":"markdown","metadata":{"id":"UVb2UtvOzLAT"},"source":["The results on the validation dataset matched expectations and the model trained on twitter sentiment analysis achieved best f1 score, so I will use it to generate predictions."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["85cbceee500448d7ae3a59e1972a1d6c","006dc4366de64ac59e926025b2fadb76","1bbc5057749d4237afb45abd32dcc7f2","f1278fa2a5124f1894843b319d298c99","717cd88ab73e4d3fada3e48a51743f17","628517d2553b41f3a2bbda8b2487dd35","5ee9ade599c640d79f3c3f5579696907","542634e593e743748d8b4c526d7cf250","f582367a8fd2499eaf9f26fb4e4f2e68","368ed9f75ddc44a593a76e4ae3f2599d","8b3ca776a82a4553ac07d8d60b419068"]},"id":"4mYPkHiywr1M","executionInfo":{"status":"ok","timestamp":1638461355519,"user_tz":300,"elapsed":58008,"user":{"displayName":"Alaa Boukhary","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11668845938527693120"}},"outputId":"ddc36c94-38a8-4b69-b55c-117fa5b00c84"},"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer\n","from datasets import ClassLabel\n","from datasets import Dataset\n","from transformers import AutoModelForSequenceClassification\n","from transformers import TrainingArguments\n","from transformers import Trainer\n","import numpy as np\n","from datasets import load_metric\n","import pandas as pd\n","\n","def tokenize_test_function(examples):\n","    tokens = tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True)\n","    return tokens\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, max_length= 512, references=labels,average=\"weighted\")\n","\n","model_path = \"/content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/\"\n","data_path = \"/content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/data/\"\n","labels = ClassLabel(num_classes=3, names=['negative', 'positive','neutral'])\n","tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n","test_dataset = load_dataset('csv', data_files= data_path+'test.csv')\n","full_test_set = test_dataset.map(tokenize_test_function, batched=True)['train'].shuffle(seed=42)\n","model = AutoModelForSequenceClassification.from_pretrained(model_path+\"cardiffnlp/twitter-xlm-roberta-base-sentiment\", num_labels=3)\n","training_args = TrainingArguments(\"test_trainer\")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=full_test_set,\n","    eval_dataset=full_test_set,\n","    compute_metrics=compute_metrics\n",")\n","predictions,_,_ = trainer.predict(full_test_set)\n","predictions = np.argmax(predictions, axis=1)\n","test_df = pd.read_csv(data_path + \"test.csv\")\n","test_df[\"sentiment\"] = labels.int2str(predictions)\n","test_df.to_csv(data_path+\"predictions.csv\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.5\",\n","  \"vocab_size\": 119547\n","}\n","\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/28e5b750bf4f39cc620367720e105de1501cf36ec4ca7029eba82c1d2cc47caf.6c5b6600e968f4b5e08c86d8891ea99e51537fc2bf251435fb46922e8f7a7b29\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/5cbdf121f196be5f1016cb102b197b0c34009e1e658f513515f2eebef9f38093.b33e51591f94f17c238ee9b1fac75b96ff2678cbaed6e108feadb3449d18dc24\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/47087d99feeb3bc6184d7576ff089c52f7fbe3219fe48c6c4fa681e617753256.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n","loading configuration file https://huggingface.co/distilbert-base-multilingual-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/cf37a9dc282a679f121734d06f003625d14cfdaf55c14358c4c0b8e7e2b89ac9.7a727bd85e40715bec919a39cdd6f0aba27a8cd488f2d4e0f512448dcd02bf0f\n","Model config DistilBertConfig {\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForMaskedLM\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"initializer_range\": 0.02,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"transformers_version\": \"4.12.5\",\n","  \"vocab_size\": 119547\n","}\n","\n","Using custom data configuration default-f69c411cacf2f64b\n","Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-f69c411cacf2f64b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"85cbceee500448d7ae3a59e1972a1d6c","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-f69c411cacf2f64b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-6c1c285e0d0ea6a3.arrow\n","Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/csv/default-f69c411cacf2f64b/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-2001893b44499dc0.arrow\n","loading configuration file /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/config.json\n","Model config XLMRobertaConfig {\n","  \"_name_or_path\": \"cardiffnlp/twitter-xlm-roberta-base-sentiment\",\n","  \"architectures\": [\n","    \"XLMRobertaForSequenceClassification\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"id2label\": {\n","    \"0\": \"Negative\",\n","    \"1\": \"Neutral\",\n","    \"2\": \"Positive\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"label2id\": {\n","    \"Negative\": 0,\n","    \"Neutral\": 1,\n","    \"Positive\": 2\n","  },\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"xlm-roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"problem_type\": \"single_label_classification\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.12.5\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 250002\n","}\n","\n","loading weights file /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/pytorch_model.bin\n","All model checkpoint weights were used when initializing XLMRobertaForSequenceClassification.\n","\n","All the weights of XLMRobertaForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/models/cardiffnlp/twitter-xlm-roberta-base-sentiment.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForSequenceClassification for predictions without further training.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","The following columns in the test set  don't have a corresponding argument in `XLMRobertaForSequenceClassification.forward` and have been ignored: content.\n","***** Running Prediction *****\n","  Num examples = 2500\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [313/313 00:49]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"dDLdrQT5BGQj"},"source":["# Generate Requirements.txt"]},{"cell_type":"code","metadata":{"id":"X8DHKFqmBsXA","executionInfo":{"status":"ok","timestamp":1638461650191,"user_tz":300,"elapsed":1739,"user":{"displayName":"Alaa Boukhary","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11668845938527693120"}}},"source":["!pip freeze > '/content/drive/MyDrive/Synthesio-test/sentiment-analysis-test/requirements.txt'"],"execution_count":11,"outputs":[]}]}